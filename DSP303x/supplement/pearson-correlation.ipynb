{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12: Xây dựng mô hình hồi quy tuyến tính của riêng bạn\n",
    "\n",
    "Tùy ý chọn đặc trưng và điều chỉnh siêu tham số\n",
    "\n",
    "Yêu cầu: R2 score >= 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to have a correlation matrix and use it to decide which feature to use based on 2 criteria\n",
    "\n",
    "1. Pearson's Correlation coefficient $(r)$ in respected to \"Accept\" must be: $$|r| >= 0.4$$\n",
    "because $|r|$ between 0.3 and 0.5 is consider moderate to high correlation, and we want to contain some other moderate to high correlation feature to avoid overfitting\n",
    "- Ridge Regression\n",
    "\n",
    "2. In order to avoid multicolinality, each feature correlation with each other must be:\n",
    "$$|r| < 0.5$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7779349393330621,\n",
       " ['Apps', 'Enroll'],\n",
       "             Apps    Enroll\n",
       " Apps    1.000000  0.846822\n",
       " Enroll  0.846822  1.000000]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def hai_model():\n",
    "    # First, let's take the Feature and correlation ef r that satisfy abs(r) >= 0.4\n",
    "    # feature_condition_1 = (\n",
    "    #     df_corr[\"Accept\"]\n",
    "    #     .loc[df_corr[\"Accept\"].abs() >= 0.4]\n",
    "    #     .drop(index=\"Accept\")\n",
    "    # )\n",
    "    # feature_condition_1 = feature_condition_1.index.to_list()\n",
    "    feature_condition_1 = [\"Apps\", \"Enroll\"]\n",
    "    # After that, let's check the correlation matrix for those feature\n",
    "    df_corr_hai_model = df_corr.loc[feature_condition_1, feature_condition_1]\n",
    "    # df_corr_hai_model < 0.5\n",
    "    # df_corr_hai_model.abs() < 0.5\n",
    "\n",
    "    # Take the feature and apply it into train-set split\n",
    "    X_train_hai = np.asanyarray(X_train[feature_condition_1])\n",
    "    y_train_hai = np.asanyarray(y_train)\n",
    "    X_test_hai = np.asanyarray(X_test[feature_condition_1])\n",
    "    y_test_hai = np.asanyarray(y_test)\n",
    "\n",
    "    # # Normalization\n",
    "    # scaler = StandardScaler()\n",
    "    # scaler.fit(X=X_train_hai)\n",
    "    # X_train_hai = scaler.transform(X=X_train_hai)\n",
    "\n",
    "    # scaler.fit(X=X_test_hai)\n",
    "    # X_test_hai = scaler.transform(X=X_test_hai)\n",
    "\n",
    "    # # PCA\n",
    "    # n_components = 1\n",
    "    # pca = PCA(n_components=n_components)\n",
    "\n",
    "    # X_train_hai = pca.fit_transform(X_train_hai)\n",
    "\n",
    "    # X_test_hai = pca.fit_transform(X_test_hai)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X=X_train_hai, y=y_train_hai)\n",
    "\n",
    "    # Calculate by score\n",
    "    score_ = model.score(X=X_test_hai, y=y_test_hai)\n",
    "\n",
    "    # return pca.explained_variance_ratio_\n",
    "    return [score_, feature_condition_1, df_corr_hai_model]\n",
    "\n",
    "\n",
    "hai_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
